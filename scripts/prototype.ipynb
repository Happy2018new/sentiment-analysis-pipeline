{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c6efe6",
   "metadata": {},
   "source": [
    "## Read File\n",
    "\n",
    "First, we need to read the raw data from `sample_stream.jsonl`.<br/>\n",
    "Due to each line holds a single JSON object, so we read one line by line util the end of this file.<br/>\n",
    "For each line, we use `json.loads` to load this object as **Python** `dict` object, and append this object to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "92d0bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestamp': '2025-05-01T10:00:00', 'text': \"I love this product! It's absolutely fantastic.\"}\n",
      "{'timestamp': '2025-05-01T10:05:00', 'text': 'Not what I expected. Pretty disappointed.'}\n",
      "{'timestamp': '2025-05-01T10:10:00', 'text': \"It's okay, does the job. Nothing special though.\"}\n",
      "{'timestamp': '2025-05-01T10:15:00', 'text': 'Terrible experience. Would not recommend!'}\n",
      "{'timestamp': '2025-05-01T10:20:00', 'text': 'Absolutely brilliant. Exceeded my expectations.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# This is a list that hold each comment.\n",
    "# Each element is a dict that parsed from a single JSON line.\n",
    "streams: list[dict] = []\n",
    "\n",
    "# Open the file with read mode and utf-8 encoding.\n",
    "# Then, read each line by each line, and append the the list.\n",
    "with open(\"../data/sample_stream.jsonl\", \"r+\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        streams.append(json.loads(line))\n",
    "\n",
    "# Print each comment.\n",
    "for i in streams:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c06fd0e",
   "metadata": {},
   "source": [
    "## Initialize NLTK module\n",
    "\n",
    "First we need to download the things that **NLTK** module necessary needed.<br/>\n",
    "We running the following code to download them and check everything is OK.\n",
    "\n",
    "Note that before import nltk, you need to make sure you already use **PIP** to install this module, because this is a third party module.<br>\n",
    "You can use the following statements to install `nltk`. Please make sure you running this in the shell.\n",
    "\n",
    "```shell\n",
    "pip install nltk --upgrade\n",
    "```\n",
    "\n",
    "The `upgrade` argument is to make sure you will upgrade to the latest release.<br>\n",
    "Just for those who already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "40171b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Happy2018new\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     C:\\Users\\Happy2018new\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\Happy2018new\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     C:\\Users\\Happy2018new\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\Happy2018new\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Happy2018new\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Happy2018new\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Happy2018new\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Happy2018new\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Happy2018new\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download all the book we need.\n",
    "nltk.download(\"gutenberg\")\n",
    "nltk.download(\"genesis\")\n",
    "nltk.download(\"inaugural\")\n",
    "nltk.download(\"nps_chat\")\n",
    "nltk.download(\"webtext\")\n",
    "nltk.download(\"treebank\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "# If all books are downloaded,\n",
    "# then this line will have no error.\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f346ed8",
   "metadata": {},
   "source": [
    "If no error was found, then we finish the preparation part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dda17c",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "Now we can start the first step of pre-processing the data.<br/>\n",
    "In the very beginning we should to do is to **tokenize** the users' comments.\n",
    "\n",
    "To make the system to be stronger, or in the other words, to make it highly robust and suitable for most situations, we first need to do is sentence tokenize.\n",
    "\n",
    "We use the following codes to this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d75f3be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment 1:\n",
      "\tI love this product!\n",
      "\tIt's absolutely fantastic.\n",
      "\n",
      "Comment 2:\n",
      "\tNot what I expected.\n",
      "\tPretty disappointed.\n",
      "\n",
      "Comment 3:\n",
      "\tIt's okay, does the job.\n",
      "\tNothing special though.\n",
      "\n",
      "Comment 4:\n",
      "\tTerrible experience.\n",
      "\tWould not recommend!\n",
      "\n",
      "Comment 5:\n",
      "\tAbsolutely brilliant.\n",
      "\tExceeded my expectations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The result of \"nltk.sent_tokenize\" is a list of strings.\n",
    "# And because we have multiple comments, so here we use\n",
    "# `list[list[str]]` as the type hint.\n",
    "sent_tokenize: list[list[str]] = []\n",
    "\n",
    "# For each comment in the list,\n",
    "# we do sentence tokenization and append the result to the list.\n",
    "for i in streams:\n",
    "    text: str = i[\"text\"]\n",
    "    sent_tokenize.append(nltk.sent_tokenize(text))\n",
    "\n",
    "# To ensure the result match our expected,\n",
    "# We print the tokenized sentences.\n",
    "for index, sentences in enumerate(sent_tokenize):\n",
    "    print(f\"Comment {index+1}:\")\n",
    "    for sentence in sentences:\n",
    "        print(\"\\t\" + sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa167299",
   "metadata": {},
   "source": [
    "Then the next step is to do word tokenize for each sentence.<br/>\n",
    "To make everything in organized, I create a simple class named `Sentences` to store the data.\n",
    "\n",
    "A single `Sentences` instance holds the following information.\n",
    "- The origin comment (that this comment can have 1 or many sentences)\n",
    "- Each sentence of this comment\n",
    "- Each token for each sentence.\n",
    "\n",
    "Use `__repr__` can show all these things more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fb8c2361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment 1:\n",
      "\tSentences(origin_text=\"I love this product! It's absolutely fantastic.\", sent_tokens=['I love this product!', \"It's absolutely fantastic.\"], word_tokens=[['I', 'love', 'this', 'product', '!'], ['It', \"'s\", 'absolutely', 'fantastic', '.']])\n",
      "\n",
      "Comment 2:\n",
      "\tSentences(origin_text=\"Not what I expected. Pretty disappointed.\", sent_tokens=['Not what I expected.', 'Pretty disappointed.'], word_tokens=[['Not', 'what', 'I', 'expected', '.'], ['Pretty', 'disappointed', '.']])\n",
      "\n",
      "Comment 3:\n",
      "\tSentences(origin_text=\"It's okay, does the job. Nothing special though.\", sent_tokens=[\"It's okay, does the job.\", 'Nothing special though.'], word_tokens=[['It', \"'s\", 'okay', ',', 'does', 'the', 'job', '.'], ['Nothing', 'special', 'though', '.']])\n",
      "\n",
      "Comment 4:\n",
      "\tSentences(origin_text=\"Terrible experience. Would not recommend!\", sent_tokens=['Terrible experience.', 'Would not recommend!'], word_tokens=[['Terrible', 'experience', '.'], ['Would', 'not', 'recommend', '!']])\n",
      "\n",
      "Comment 5:\n",
      "\tSentences(origin_text=\"Absolutely brilliant. Exceeded my expectations.\", sent_tokens=['Absolutely brilliant.', 'Exceeded my expectations.'], word_tokens=[['Absolutely', 'brilliant', '.'], ['Exceeded', 'my', 'expectations', '.']])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Sentences:\n",
    "    origin_text: str\n",
    "    sent_tokens: list[str]\n",
    "    word_tokens: list[list[str]]\n",
    "\n",
    "    def __init__(self, text: str) -> None:\n",
    "        self.origin_text = text\n",
    "        self.sent_tokens = nltk.sent_tokenize(self.origin_text)\n",
    "        self.word_tokens = [nltk.word_tokenize(i) for i in self.sent_tokens]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        result = \"Sentences(\"\n",
    "        result += f\"origin_text={json.dumps(self.origin_text,ensure_ascii=False)}, \"\n",
    "        result += f\"sent_tokens={self.sent_tokens}, \"\n",
    "        result += f\"word_tokens={self.word_tokens}\"\n",
    "        return result + \")\"\n",
    "\n",
    "\n",
    "comments = [Sentences(i[\"text\"]) for i in streams]\n",
    "for index, comment in enumerate(comments):\n",
    "    print(f\"Comment {index+1}:\")\n",
    "    print(f\"\\t{comment}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f671204",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Now we finish the tokenize processing step.\n",
    "\n",
    "However, we want to state the appear count for each word, but you can simply think about this...\n",
    "- play\n",
    "- plays\n",
    "- playing\n",
    "- played\n",
    "- player\n",
    "- playful\n",
    "\n",
    "Well, these words look the same, but different in **Python** because they have different characters so that they are different strings.<br/>\n",
    "But we wish when we count these words, we can make they are the same, so the result count is 6 but not each word get count 1.\n",
    "\n",
    "So the next step we need to do is to **Stemming** the tokens, to stem each word tokens so that it can be more convenient for me to state the words with different frequencies of occurrence and the same root will be classified into the same group.\n",
    "\n",
    "Then, I can compute the distance between different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "40c6a7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment 1:\n",
      "\tStemSentences(origin_text=\"I love this product! It's absolutely fantastic.\", sent_tokens=['I love this product!', \"It's absolutely fantastic.\"], word_tokens=[['I', 'love', 'this', 'product', '!'], ['It', \"'s\", 'absolutely', 'fantastic', '.']], stem_tokens=[['i', 'love', 'thi', 'product', '!'], ['it', \"'s\", 'absolut', 'fantast', '.']])\n",
      "\n",
      "Comment 2:\n",
      "\tStemSentences(origin_text=\"Not what I expected. Pretty disappointed.\", sent_tokens=['Not what I expected.', 'Pretty disappointed.'], word_tokens=[['Not', 'what', 'I', 'expected', '.'], ['Pretty', 'disappointed', '.']], stem_tokens=[['not', 'what', 'i', 'expect', '.'], ['pretti', 'disappoint', '.']])\n",
      "\n",
      "Comment 3:\n",
      "\tStemSentences(origin_text=\"It's okay, does the job. Nothing special though.\", sent_tokens=[\"It's okay, does the job.\", 'Nothing special though.'], word_tokens=[['It', \"'s\", 'okay', ',', 'does', 'the', 'job', '.'], ['Nothing', 'special', 'though', '.']], stem_tokens=[['it', \"'s\", 'okay', ',', 'doe', 'the', 'job', '.'], ['noth', 'special', 'though', '.']])\n",
      "\n",
      "Comment 4:\n",
      "\tStemSentences(origin_text=\"Terrible experience. Would not recommend!\", sent_tokens=['Terrible experience.', 'Would not recommend!'], word_tokens=[['Terrible', 'experience', '.'], ['Would', 'not', 'recommend', '!']], stem_tokens=[['terribl', 'experi', '.'], ['would', 'not', 'recommend', '!']])\n",
      "\n",
      "Comment 5:\n",
      "\tStemSentences(origin_text=\"Absolutely brilliant. Exceeded my expectations.\", sent_tokens=['Absolutely brilliant.', 'Exceeded my expectations.'], word_tokens=[['Absolutely', 'brilliant', '.'], ['Exceeded', 'my', 'expectations', '.']], stem_tokens=[['absolut', 'brilliant', '.'], ['exceed', 'my', 'expect', '.']])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "DEFAULT_PORTER_STEMMER = PorterStemmer()\n",
    "\n",
    "\n",
    "class StemSentences(Sentences):\n",
    "    stem_tokens: list[list[str]]\n",
    "\n",
    "    def __init__(self, text: str) -> None:\n",
    "        super().__init__(text)\n",
    "        self.stem_tokens = [\n",
    "            [DEFAULT_PORTER_STEMMER.stem(j) for j in i] for i in self.word_tokens\n",
    "        ]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        result = \"StemSentences(\"\n",
    "        result += f\"origin_text={json.dumps(self.origin_text,ensure_ascii=False)}, \"\n",
    "        result += f\"sent_tokens={self.sent_tokens}, \"\n",
    "        result += f\"word_tokens={self.word_tokens}, \"\n",
    "        result += f\"stem_tokens={self.stem_tokens}\"\n",
    "        return result + \")\"\n",
    "\n",
    "\n",
    "stem_comments = [StemSentences(i[\"text\"]) for i in streams]\n",
    "for index, comment in enumerate(stem_comments):\n",
    "    print(f\"Comment {index+1}:\")\n",
    "    print(f\"\\t{comment}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc95d46e",
   "metadata": {},
   "source": [
    "I create a new class named `StemSentences` that based on `Sentences`.<br/>\n",
    "This new class has a new field named `stem_tokens` to store the stemming results for word tokens of each sentence.\n",
    "\n",
    "`PorterStemmer` is the stemmer I used because this is widely used in English.<br/>\n",
    "In addition, due to I'll not to change the stemmer, so use a constant `DEFAULT_PORTER_STEMMER` for it, so that we don't need to initialize stemmer by `PorterStemmer()` for each call of `stem`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542cf18a",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Stemming can make us simple to count the words of each comments.\n",
    "\n",
    "**Stemming** part can help to enhance the speed of compute the distance between different groups.<br/>\n",
    "But the results of **Stemming** is for the machine and hard for human to read...\n",
    "\n",
    "So, here we need **Lemmatization** to get the real English word for each token.<br/>\n",
    "That means use the results of **Stemming** to do internal compute, and use **Lemmatization** for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d3793f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment 1:\n",
      "\tStemSentences(origin_text=\"I love this product! It's absolutely fantastic.\", sent_tokens=['I love this product!', \"It's absolutely fantastic.\"], word_tokens=[['I', 'love', 'this', 'product', '!'], ['It', \"'s\", 'absolutely', 'fantastic', '.']], stem_tokens=[['i', 'love', 'thi', 'product', '!'], ['it', \"'s\", 'absolut', 'fantast', '.']], lem_tokens=[['i', 'love', 'this', 'product', '!'], ['it', \"'s\", 'absolutely', 'fantastic', '.']])\n",
      "\n",
      "Comment 2:\n",
      "\tStemSentences(origin_text=\"Not what I expected. Pretty disappointed.\", sent_tokens=['Not what I expected.', 'Pretty disappointed.'], word_tokens=[['Not', 'what', 'I', 'expected', '.'], ['Pretty', 'disappointed', '.']], stem_tokens=[['not', 'what', 'i', 'expect', '.'], ['pretti', 'disappoint', '.']], lem_tokens=[['not', 'what', 'i', 'expect', '.'], ['pretty', 'disappointed', '.']])\n",
      "\n",
      "Comment 3:\n",
      "\tStemSentences(origin_text=\"It's okay, does the job. Nothing special though.\", sent_tokens=[\"It's okay, does the job.\", 'Nothing special though.'], word_tokens=[['It', \"'s\", 'okay', ',', 'does', 'the', 'job', '.'], ['Nothing', 'special', 'though', '.']], stem_tokens=[['it', \"'s\", 'okay', ',', 'doe', 'the', 'job', '.'], ['noth', 'special', 'though', '.']], lem_tokens=[['it', \"'s\", 'okay', ',', 'do', 'the', 'job', '.'], ['nothing', 'special', 'though', '.']])\n",
      "\n",
      "Comment 4:\n",
      "\tStemSentences(origin_text=\"Terrible experience. Would not recommend!\", sent_tokens=['Terrible experience.', 'Would not recommend!'], word_tokens=[['Terrible', 'experience', '.'], ['Would', 'not', 'recommend', '!']], stem_tokens=[['terribl', 'experi', '.'], ['would', 'not', 'recommend', '!']], lem_tokens=[['terrible', 'experience', '.'], ['would', 'not', 'recommend', '!']])\n",
      "\n",
      "Comment 5:\n",
      "\tStemSentences(origin_text=\"Absolutely brilliant. Exceeded my expectations.\", sent_tokens=['Absolutely brilliant.', 'Exceeded my expectations.'], word_tokens=[['Absolutely', 'brilliant', '.'], ['Exceeded', 'my', 'expectations', '.']], stem_tokens=[['absolut', 'brilliant', '.'], ['exceed', 'my', 'expect', '.']], lem_tokens=[['absolutely', 'brilliant', '.'], ['exceeded', 'my', 'expectation', '.']])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "DEFAULT_LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "class LemmerWrapper:\n",
    "    @staticmethod\n",
    "    def get_file_key(pos_tag):\n",
    "        return {\n",
    "            \"NN\": \"n\",\n",
    "            \"VB\": \"v\",\n",
    "            \"RB\": \"r\",\n",
    "            \"JJ\": \"a\",\n",
    "        }.get(pos_tag[:2], \"n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def lemmatize(word: str) -> str:\n",
    "        tag = nltk.pos_tag([word])[0][1]\n",
    "        return DEFAULT_LEMMATIZER.lemmatize(word, LemmerWrapper.get_file_key(tag))\n",
    "\n",
    "\n",
    "class FilterSentences(Sentences):\n",
    "    stem_tokens: list[list[str]]\n",
    "    lem_tokens: list[list[str]]\n",
    "\n",
    "    def __init__(self, text: str) -> None:\n",
    "        super().__init__(text)\n",
    "        self.stem_tokens = [\n",
    "            [DEFAULT_PORTER_STEMMER.stem(j) for j in i] for i in self.word_tokens\n",
    "        ]\n",
    "        self.lem_tokens = [\n",
    "            [LemmerWrapper.lemmatize(j).lower() for j in i] for i in self.word_tokens\n",
    "        ]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        result = \"StemSentences(\"\n",
    "        result += f\"origin_text={json.dumps(self.origin_text,ensure_ascii=False)}, \"\n",
    "        result += f\"sent_tokens={self.sent_tokens}, \"\n",
    "        result += f\"word_tokens={self.word_tokens}, \"\n",
    "        result += f\"stem_tokens={self.stem_tokens}, \"\n",
    "        result += f\"lem_tokens={self.lem_tokens}\"\n",
    "        return result + \")\"\n",
    "\n",
    "\n",
    "filter_comments = [FilterSentences(i[\"text\"]) for i in streams]\n",
    "for index, comment in enumerate(filter_comments):\n",
    "    print(f\"Comment {index+1}:\")\n",
    "    print(f\"\\t{comment}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f60a7e",
   "metadata": {},
   "source": [
    "We select `WordNetLemmatizer` to do the lemmatize.<br/>\n",
    "Note that this time we'll still not change the instance of `WordNetLemmatizer` so we just use a constant to store a `WordNetLemmatizer()` so I can use it in anywhere.\n",
    "\n",
    "\n",
    "`LemmerWrapper` wrapped a lemmer. This is util for me so I can just call `LemmerWrapper.lemmatize`.<br/>\n",
    "`get_file_key` is to get the part of speech of the word based on the tag attached to nltk.\n",
    "\n",
    "I place `get_file_key` and `lemmatize` into `LemmerWrapper` to make these 2 functions organized.<br/>\n",
    "Use `staticmethod` for those two functions so that no need to create a new `LemmerWrapper` instance when calling the functions in this class.\n",
    "\n",
    "Same as `StemSentences`, this time we still based on `Sentences`, but contains 2 extra fields named:\n",
    "- `stem_tokens` (Store the stemming results)\n",
    "- `lem_tokens` (Store the lemming results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b62c0",
   "metadata": {},
   "source": [
    "## Clean Stop Words\n",
    "\n",
    "These words are some example of stop words in English.\n",
    "- a\n",
    "- an\n",
    "- the\n",
    "- and\n",
    "- or\n",
    "\n",
    "And these words actually just have a little contribution to the sentence.<br/>\n",
    "However, we store these words in the tokens and count them, and it is not good for us.\n",
    "\n",
    "Therefore, we should clean these stop words from the tokens.<br/>\n",
    "Here let we do that, by create a new class named `StopWordCleaner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7c007f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment 1:\n",
      "\tStemSentences(origin_text=\"I love this product! It's absolutely fantastic.\", sent_tokens=['I love this product!', \"It's absolutely fantastic.\"], word_tokens=[['I', 'love', 'this', 'product', '!'], ['It', \"'s\", 'absolutely', 'fantastic', '.']], stem_tokens=[['', 'love', 'thi', 'product', ''], ['', \"'s\", 'absolut', 'fantast', '']], lem_tokens=[['', 'love', 'this', 'product', ''], ['', \"'s\", 'absolutely', 'fantastic', '']])\n",
      "\n",
      "Comment 2:\n",
      "\tStemSentences(origin_text=\"Not what I expected. Pretty disappointed.\", sent_tokens=['Not what I expected.', 'Pretty disappointed.'], word_tokens=[['Not', 'what', 'I', 'expected', '.'], ['Pretty', 'disappointed', '.']], stem_tokens=[['not', '', '', 'expect', ''], ['pretti', 'disappoint', '']], lem_tokens=[['not', '', '', 'expect', ''], ['pretty', 'disappointed', '']])\n",
      "\n",
      "Comment 3:\n",
      "\tStemSentences(origin_text=\"It's okay, does the job. Nothing special though.\", sent_tokens=[\"It's okay, does the job.\", 'Nothing special though.'], word_tokens=[['It', \"'s\", 'okay', ',', 'does', 'the', 'job', '.'], ['Nothing', 'special', 'though', '.']], stem_tokens=[['', \"'s\", 'okay', '', 'doe', '', 'job', ''], ['noth', 'special', 'though', '']], lem_tokens=[['', \"'s\", 'okay', '', '', '', 'job', ''], ['nothing', 'special', 'though', '']])\n",
      "\n",
      "Comment 4:\n",
      "\tStemSentences(origin_text=\"Terrible experience. Would not recommend!\", sent_tokens=['Terrible experience.', 'Would not recommend!'], word_tokens=[['Terrible', 'experience', '.'], ['Would', 'not', 'recommend', '!']], stem_tokens=[['terribl', 'experi', ''], ['would', 'not', 'recommend', '']], lem_tokens=[['terrible', 'experience', ''], ['would', 'not', 'recommend', '']])\n",
      "\n",
      "Comment 5:\n",
      "\tStemSentences(origin_text=\"Absolutely brilliant. Exceeded my expectations.\", sent_tokens=['Absolutely brilliant.', 'Exceeded my expectations.'], word_tokens=[['Absolutely', 'brilliant', '.'], ['Exceeded', 'my', 'expectations', '.']], stem_tokens=[['absolut', 'brilliant', ''], ['exceed', '', 'expect', '']], lem_tokens=[['absolutely', 'brilliant', ''], ['exceeded', '', 'expectation', '']])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "CONST_STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "CONST_STOP_WORDS |= {\",\", \".\", \"!\", \"?\", \";\", \":\", \"'\", '\"', \"`\", \"``\"}\n",
    "CONST_STOP_WORDS -= {\n",
    "    \"shan't\",\n",
    "    \"wouldn't\",\n",
    "    \"shouldn't\",\n",
    "    \"wasn't\",\n",
    "    \"aren't\",\n",
    "    \"not\",\n",
    "    \"mightn't\",\n",
    "    \"no\",\n",
    "    \"doesn't\",\n",
    "    \"hasn't\",\n",
    "    \"won't\",\n",
    "    \"isn't\",\n",
    "    \"out\",\n",
    "    \"don't\",\n",
    "    \"didn't\",\n",
    "    \"needn't\",\n",
    "    \"mustn't\",\n",
    "    \"hadn't\",\n",
    "    \"couldn't\",\n",
    "    \"off\",\n",
    "    \"nor\",\n",
    "}\n",
    "CONST_STOP_WORDS -= {\"very\", \"to\"}\n",
    "CONST_STOP_WORDS -= {\"should\", \"can\", \"will\"}\n",
    "CONST_STOP_WORDS -= {\"if\"}\n",
    "CONST_STOP_WORDS -= {\"which\", \"who\", \"this\", \"those\", \"that\", \"these\", \"whom\"}\n",
    "CONST_STOP_WORDS -= {\"each\", \"most\", \"few\", \"all\", \"some\", \"more\", \"any\"}\n",
    "CONST_STOP_WORDS -= {\"before\", \"between\", \"during\", \"against\", \"after\"}\n",
    "\n",
    "\n",
    "class StopWordCleanner:\n",
    "    @staticmethod\n",
    "    def clean(sent: FilterSentences) -> FilterSentences:\n",
    "        sent.stem_tokens = [\n",
    "            [j if j not in CONST_STOP_WORDS else \"\" for j in i]\n",
    "            for i in sent.stem_tokens\n",
    "        ]\n",
    "        sent.lem_tokens = [\n",
    "            [j if j not in CONST_STOP_WORDS else \"\" for j in i] for i in sent.lem_tokens\n",
    "        ]\n",
    "        return sent\n",
    "\n",
    "\n",
    "filter_comments = [StopWordCleanner.clean(i) for i in filter_comments]\n",
    "for index, comment in enumerate(filter_comments):\n",
    "    print(f\"Comment {index+1}:\")\n",
    "    print(f\"\\t{comment}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34841e31",
   "metadata": {},
   "source": [
    "You can noticed that I removed some words from the original set which is `set(stopwords.words(\"english\"))`, because we don't want the meaning of the sentences changed. For example, the reversal of meaning in the original sentence.\n",
    "\n",
    "Still use `staticmethod` for `StopWordCleanner.clean` so that I can use it directly without create a `StopWordCleanner` instance.\n",
    "\n",
    "In addition, with using of the following expressions, I also removed the punctuation marks from the token at the same time.\n",
    "```python\n",
    "CONST_STOP_WORDS |= {\",\", \".\", \"!\", \"?\", \";\", \":\", \"'\", '\"', \"`\", \"``\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8fe5f",
   "metadata": {},
   "source": [
    "## Compact Inverse Words\n",
    "\n",
    "We wish to compute the distance between each tokens, but you can find a token list like this:\n",
    "```python\n",
    "[\"not\", \"good\"]\n",
    "```\n",
    "\n",
    "And that means the program will think `good` and `not` have no relation, <br/>\n",
    "and just compute the distance without any thinking.\n",
    "\n",
    "But our human know this means `not good`, <br/>\n",
    "so we need to combine these two tokens to something like that:\n",
    "```python\n",
    "[\"NEG_good\"]\n",
    "```\n",
    "\n",
    "So let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2cc40d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment 1:\n",
      "\tStemSentences(origin_text=\"I love this product! It's absolutely fantastic.\", sent_tokens=['I love this product!', \"It's absolutely fantastic.\"], word_tokens=[['I', 'love', 'this', 'product', '!'], ['It', \"'s\", 'absolutely', 'fantastic', '.']], stem_tokens=[['', 'love', 'thi', 'product', ''], ['', \"'s\", 'absolut', 'fantast', '']], lem_tokens=[['', 'love', 'this', 'product', ''], ['', \"'s\", 'absolutely', 'fantastic', '']])\n",
      "\n",
      "Comment 2:\n",
      "\tStemSentences(origin_text=\"Not what I expected. Pretty disappointed.\", sent_tokens=['Not what I expected.', 'Pretty disappointed.'], word_tokens=[['Not', 'what', 'I', 'expected', '.'], ['Pretty', 'disappointed', '.']], stem_tokens=[['not', 'NEG_', 'NEG_', 'NEG_expect', ''], ['pretti', 'disappoint', '']], lem_tokens=[['not', '', '', 'expect', ''], ['pretty', 'disappointed', '']])\n",
      "\n",
      "Comment 3:\n",
      "\tStemSentences(origin_text=\"It's okay, does the job. Nothing special though.\", sent_tokens=[\"It's okay, does the job.\", 'Nothing special though.'], word_tokens=[['It', \"'s\", 'okay', ',', 'does', 'the', 'job', '.'], ['Nothing', 'special', 'though', '.']], stem_tokens=[['', \"'s\", 'okay', '', 'doe', '', 'job', ''], ['noth', 'special', 'though', '']], lem_tokens=[['', \"'s\", 'okay', '', '', '', 'job', ''], ['nothing', 'special', 'though', '']])\n",
      "\n",
      "Comment 4:\n",
      "\tStemSentences(origin_text=\"Terrible experience. Would not recommend!\", sent_tokens=['Terrible experience.', 'Would not recommend!'], word_tokens=[['Terrible', 'experience', '.'], ['Would', 'not', 'recommend', '!']], stem_tokens=[['terribl', 'experi', ''], ['would', 'not', 'NEG_recommend', 'NEG_']], lem_tokens=[['terrible', 'experience', ''], ['would', 'not', 'recommend', '']])\n",
      "\n",
      "Comment 5:\n",
      "\tStemSentences(origin_text=\"Absolutely brilliant. Exceeded my expectations.\", sent_tokens=['Absolutely brilliant.', 'Exceeded my expectations.'], word_tokens=[['Absolutely', 'brilliant', '.'], ['Exceeded', 'my', 'expectations', '.']], stem_tokens=[['absolut', 'brilliant', ''], ['exceed', '', 'expect', '']], lem_tokens=[['absolutely', 'brilliant', ''], ['exceeded', '', 'expectation', '']])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class InverseCompacter:\n",
    "    WINDOW_SIZE = 3\n",
    "    NEG_TOKENS = {\n",
    "        \"won't\",\n",
    "        \"n't\",\n",
    "        \"out\",\n",
    "        \"without\",\n",
    "        \"no\",\n",
    "        \"don't\",\n",
    "        \"mightn't\",\n",
    "        \"isn't\",\n",
    "        \"doesn't\",\n",
    "        \"shouldn't\",\n",
    "        \"can't\",\n",
    "        \"wouldn't\",\n",
    "        \"hadn't\",\n",
    "        \"nor\",\n",
    "        \"off\",\n",
    "        \"cannot\",\n",
    "        \"needn't\",\n",
    "        \"never\",\n",
    "        \"shan't\",\n",
    "        \"didn't\",\n",
    "        \"couldn't\",\n",
    "        \"mustn't\",\n",
    "        \"not\",\n",
    "        \"aren't\",\n",
    "        \"hasn't\",\n",
    "        \"wasn't\",\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def compact_tokens(tokens: list[str]) -> list[str]:\n",
    "        result = []\n",
    "        count = 0\n",
    "\n",
    "        for token in tokens:\n",
    "            token = token.lower()\n",
    "            if token in InverseCompacter.NEG_TOKENS:\n",
    "                count = InverseCompacter.WINDOW_SIZE\n",
    "                result.append(token)\n",
    "                continue\n",
    "            if count > 0:\n",
    "                result.append(f\"NEG_{token}\")\n",
    "                count -= 1\n",
    "            else:\n",
    "                result.append(token)\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def compact_sentences(sent: FilterSentences) -> FilterSentences:\n",
    "        sent.stem_tokens = [\n",
    "            InverseCompacter.compact_tokens(i) for i in sent.stem_tokens\n",
    "        ]\n",
    "        return sent\n",
    "\n",
    "\n",
    "filter_comments = [InverseCompacter.compact_sentences(i) for i in filter_comments]\n",
    "for index, comment in enumerate(filter_comments):\n",
    "    print(f\"Comment {index+1}:\")\n",
    "    print(f\"\\t{comment}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652ee78",
   "metadata": {},
   "source": [
    "Note that this is only need to the `stem_tokens` field.<br/>\n",
    "`lem_tokens` no need that because it is for human to see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e4105d",
   "metadata": {},
   "source": [
    "## Machine Token to Human Readable Token\n",
    "\n",
    "Due to at last we need to convert our analysis results to the word that human can read, so before we start to compute the distance, we first is to create a map that can find the human readable words by given machine token.\n",
    "\n",
    "Note that for those who starts with `NEG_` like `NEG_good`, we should the corresponding human readable word to `(negative) ...` like `(negative) good`.\n",
    "\n",
    "Now let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "32d6dba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love => love\n",
      "thi => this\n",
      "product => product\n",
      "'s => 's\n",
      "absolut => absolutely\n",
      "fantast => fantastic\n",
      "not => not\n",
      "NEG_expect => (negative) expect\n",
      "pretti => pretty\n",
      "disappoint => disappointed\n",
      "okay => okay\n",
      "job => job\n",
      "noth => nothing\n",
      "special => special\n",
      "though => though\n",
      "terribl => terrible\n",
      "experi => experience\n",
      "would => would\n",
      "NEG_recommend => (negative) recommend\n",
      "brilliant => brilliant\n",
      "exceed => exceeded\n",
      "expect => expectation\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StemToLemMapping:\n",
    "    mapping: dict[str, str] = field(default_factory=lambda: {})\n",
    "\n",
    "    def build_mapping(self, sentences: list[FilterSentences]) -> StemToLemMapping:\n",
    "        for sentence in sentences:\n",
    "            for i, tokens in enumerate(sentence.stem_tokens):\n",
    "                for j, token in enumerate(tokens):\n",
    "                    key, value = token, sentence.lem_tokens[i][j]\n",
    "                    if len(token) == 0 or len(value) == 0:\n",
    "                        continue\n",
    "                    if key in self.mapping:\n",
    "                        continue\n",
    "                    self.mapping[key] = value\n",
    "        return self\n",
    "\n",
    "    def get_lem_token(self, stem_token: str) -> str:\n",
    "        result = self.mapping.get(stem_token, stem_token)\n",
    "        if stem_token.startswith(\"NEG_\"):\n",
    "            return f\"(negative) {result}\"\n",
    "        return result\n",
    "\n",
    "\n",
    "mapping = StemToLemMapping().build_mapping(filter_comments)\n",
    "for key, value in mapping.mapping.items():\n",
    "    print(f\"{key} => {mapping.get_lem_token(key)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba97c7",
   "metadata": {},
   "source": [
    "Module `dataclass` can help us easily to manage the class field.<br/>\n",
    "`field(default_factory=lambda: {})` is to ensure if the user not given a dict, then create a new empty dict for the `mapping` field."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
